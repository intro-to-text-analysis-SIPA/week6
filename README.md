# Week 6: Principles of text analysis: Cleaning and processing text for analysis with the NLTK library


This week, we will work with the Natural Language Toolkit (NLTK), a suite of Python libraries for processing and manipulating text-data. We will also review the necessary steps for cleaning and processing our text before we can analyze it by essentially converting each word in a text into an individual data point. 

- Topics covered:
    - Working with text-as-data
    - Cleaning and standardizing text data
    - Preparing texts for computational analysis
    - Basic text analysis tools 
- Curriculum for this session:
    - DHRI's [Introduction to Text Analysis with Python and NLTK](https://curriculum.dhinstitutes.org/workshops/text-analysis/) (lessons 1-12)
- Assignments (due by 10am on the day of class):
   - In a [new Jupyter Notebook](https://www.edlitera.com/en/blog/posts/g-uide-how-to-start-jupyter-notebook), practice the commands in the curriculum for this session 
    - "Gist" your Jupyter Notebook to your GitHub account 
    - Share the gist link in the weekly GitHub assignment link
- Additional readings/resources (not required, but useful!):
    - Tutorials:
        - Geeks for Geeks: [Generating Word Cloud in Python](https://www.geeksforgeeks.org/generating-word-cloud-python/#:~:text=For%20generating%20word%20cloud%20in,from%20UCI%20Machine%20Learning%20Repository)
    - Explainer:
        - Scribbr: [Textual Analysis | Guide, 3 Approaches & Examples](https://www.scribbr.com/methodology/textual-analysis/)
    - Projects:
        - Digital Humanities at Yale University Library: [Robots Reading Vogue](http://dh.library.yale.edu/projects/vogue/)
        - Boston College Library: [Text and Data Mining Projects](https://libguides.bc.edu/textdatamining/projects)
